# GPT2
gpt2:
    embedding: "transformer.wte.weight" # String. Directly accessed
    activations: # Regex pattern
    - 'mlp\.c_proj'
gpt2-medium:
    embedding: "transformer.wte.weight"
    activations:
    - 'mlp\.c_proj'
gpt2-large:
    embedding: "transformer.wte.weight"
    activations:
    - 'attn\.c_attn'
    - 'attn\.c_proj'
    - 'mlp\.c_fc'
    - 'mlp\.c_proj'
    - 'lm_head'
gpt2-xl:
    embedding: "transformer.wte.weight"
    activations:
    - 'mlp\.c_proj'
distilgpt2:
    embedding: "transformer.wte.weight"
    activations:
    - 'mlp\.c_proj'
# BERT
distilbert-base-uncased:
    embedding: "embeddings.word_embeddings"
    activations:
    - 'ffn\.lin2'
bert-base-uncased:
    embedding: "embeddings.word_embeddings"
    activations:
    - '\d+\.output\.dense' #To set apart from attention.output.dense
bert-large-uncased:
      embedding: "embeddings.word_embeddings"
      activations:
      - '\d+\.output\.dense'
'distilbert-base-uncased-finetuned-sst-2-english':
    embedding: "embeddings.word_embeddings"
    activations:
    - 'ffn.lin2'
'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract':
    embedding: "embeddings.word_embeddings"
    activations:
    - '\d+\.output\.dense'
# ALBERT
albert-base-v2:
      embedding: "embeddings.word_embeddings"
      activations:
      - '\.ffn_output'
albert-large-v2:
      embedding: "embeddings.word_embeddings"
      activations:
      - '\.ffn_output'
albert-xxlarge-v2:
      embedding: "embeddings.word_embeddings"
      activations:
      - '\.ffn_output'
# ROBERTA
roberta-base:
  embedding: 'embeddings.word_embeddings'
  activations:
    - '\d+\.output\.dense'
roberta-large:
    embedding: 'embeddings.word_embeddings'
    activations:
    - '\d+\.output\.dense'
# ELECTRA
'google/electra-small-discriminator':
    embedding: 'embeddings.word_embeddings'
    activations:
    - '\d+\.output\.dense'
'google/electra-base-discriminator':
    embedding: 'embeddings.word_embeddings'
    activations:
    - '\d+\.output\.dense'
# ENCODER DECODER MODELS
# T5
'valhalla/t5-small-qg-hl':
   embedding: 'shared'
   activations:
   - 'wo' #Note that this will be both encoder and decoder layers
'valhalla/t5-small-qa-qg-hl':
   embedding: 'shared'
   activations:
   - 'wo' #Note that this will be both encoder and decoder layers
'valhalla/t5-base-e2e-qg':
   embedding: 'shared'
   activations:
   - 'wo' #Note that this will be both encoder and decoder layers
'sentence-transformers/distilbert-base-nli-stsb-mean-tokens':
    embedding: "embeddings.word_embeddings"
    activations:
    - 'ffn.lin2'
t5-small:
    embedding: 'shared'
    activations:
    - 'wo' #Note that this will be both encoder and decoder layers
t5-base:
    embedding: 'shared'
    activations:
    - 'wo' #Note that this will be both encoder and decoder layers
# BART
facebook/bart-large-mnli:
    embedding: 'shared'
    activations:
    - 'fc2'
# DUMMY MODELS
'sshleifer/tiny-gpt2':
  embedding: "transformer.wte.weight"
  activations:
    - 'mlp.c_proj'
'julien-c/bert-xsmall-dummy':
  embedding: "embeddings.word_embeddings"
  activations:
    - '\d+\.output\.dense'

